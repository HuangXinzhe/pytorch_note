{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch模型保存与读取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 模型存储格式\n",
    "PyTorch存储模型主要采用pkl，pt，pth三种格式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 模型存储内容\n",
    "一个PyTorch模型主要包含两个部分：\n",
    "- 模型结构\n",
    "- 权重\n",
    "\n",
    "其中模型是继承nn.Module的类，权重的数据结构是一个字典（key是层名，value是权重向量）。存储也由此分为两种形式：存储整个模型（包括结构和权重），和只存储模型权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "model = models.resnet152(pretrained=True)\n",
    "save_dir = './resnet152.pth'\n",
    "\n",
    "# 保存整个模型\n",
    "torch.save(model, save_dir)\n",
    "# 保存模型权重\n",
    "torch.save(model.state_dict, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于PyTorch而言，pt, pth和pkl三种数据格式均支持模型权重和整个模型的存储，因此使用上没有差别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 单卡和多卡模型存储的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # 如果是多卡改成类似0,1,2\n",
    "model = model.cuda()  # 单卡\n",
    "model = torch.nn.DataParallel(model).cuda()  # 多卡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单卡保存+单卡加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用os.envision命令指定使用的GPU后，即可进行模型保存和读取操作。注意这里即便保存和读取时使用的GPU不同也无妨。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'   #这里替换成希望使用的GPU编号\n",
    "model = models.resnet152(pretrained=True)\n",
    "model.cuda()\n",
    "\n",
    "save_dir = 'resnet152.pt'   #保存路径\n",
    "\n",
    "# 保存+读取整个模型\n",
    "torch.save(model, save_dir)\n",
    "loaded_model = torch.load(save_dir)\n",
    "loaded_model.cuda()\n",
    "\n",
    "# 保存+读取模型权重\n",
    "torch.save(model.state_dict(), save_dir)\n",
    "loaded_model = models.resnet152()   #注意这里需要对模型结构有定义\n",
    "loaded_model.load_state_dict(torch.load(save_dir))\n",
    "loaded_model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单卡保存+多卡加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种情况的处理比较简单，读取单卡保存的模型后，使用nn.DataParallel函数进行分布式训练设置即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'   #这里替换成希望使用的GPU编号\n",
    "model = models.resnet152(pretrained=True)\n",
    "model.cuda()\n",
    "\n",
    "# 保存+读取整个模型\n",
    "torch.save(model, save_dir)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'   #这里替换成希望使用的GPU编号\n",
    "loaded_model = torch.load(save_dir)\n",
    "loaded_model = nn.DataParallel(loaded_model).cuda()\n",
    "\n",
    "# 保存+读取模型权重\n",
    "torch.save(model.state_dict(), save_dir)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'   #这里替换成希望使用的GPU编号\n",
    "loaded_model = models.resnet152()   #注意这里需要对模型结构有定义\n",
    "loaded_model.load_state_dict(torch.load(save_dir))\n",
    "loaded_model = nn.DataParallel(loaded_model).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多卡保存+单卡加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种情况下的核心问题是：如何去掉权重字典键名中的\"module\"，以保证模型的统一性。\n",
    "\n",
    "对于加载整个模型，直接提取模型的module属性即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'   #这里替换成希望使用的GPU编号\n",
    "\n",
    "model = models.resnet152(pretrained=True)\n",
    "model = nn.DataParallel(model).cuda()\n",
    "\n",
    "# 保存+读取整个模型\n",
    "torch.save(model, save_dir)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'   #这里替换成希望使用的GPU编号\n",
    "loaded_model = torch.load(save_dir).module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于加载模型权重，有以下几种思路： 保存模型时保存模型的module属性对应的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'   #这里替换成希望使用的GPU编号\n",
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "save_dir = 'resnet152.pth'   #保存路径\n",
    "model = models.resnet152(pretrained=True)\n",
    "model = nn.DataParallel(model).cuda()\n",
    "\n",
    "# 保存权重\n",
    "torch.save(model.module.state_dict(), save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'   #这里替换成希望使用的GPU编号\n",
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "model = models.resnet152(pretrained=True)\n",
    "model = nn.DataParallel(model).cuda()\n",
    "\n",
    "# 保存+读取模型权重\n",
    "torch.save(model.state_dict(), save_dir)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'   #这里替换成希望使用的GPU编号\n",
    "loaded_model = models.resnet152()   #注意这里需要对模型结构有定义\n",
    "loaded_model.load_state_dict(torch.load(save_dir))\n",
    "loaded_model = nn.DataParallel(loaded_model).cuda()\n",
    "# loaded_model.state_dict = loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历字典去除module\n",
    "from collections import OrderedDict\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'   #这里替换成希望使用的GPU编号\n",
    "\n",
    "loaded_dict = torch.load(save_dir)\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in loaded_dict.items():\n",
    "    name = k[7:] # module字段在最前面，从第7个字符开始就可以去掉module\n",
    "    new_state_dict[name] = v #新字典的key值对应的value一一对应\n",
    "\n",
    "loaded_model = models.resnet152()   #注意这里需要对模型结构有定义\n",
    "loaded_model.state_dict = new_state_dict\n",
    "loaded_model = loaded_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用replace操作去除module\n",
    "loaded_model = models.resnet152()    \n",
    "loaded_dict = torch.load(save_dir)\n",
    "loaded_model.load_state_dict({k.replace('module.', ''): v for k, v in loaded_dict.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多卡保存+多卡加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于是模型保存和加载都使用的是多卡，因此不存在模型层名前缀不同的问题。但多卡状态下存在一个device（使用的GPU）匹配的问题，即保存整个模型时会同时保存所使用的GPU id等信息，读取时若这些信息和当前使用的GPU信息不符则可能会报错或者程序不按预定状态运行。具体表现为以下两点：\n",
    "\n",
    "读取整个模型再使用nn.DataParallel进行分布式训练设置\n",
    "\n",
    "这种情况很可能会造成保存的整个模型中GPU id和读取环境下设置的GPU id不符，训练时数据所在device和模型所在device不一致而报错。\n",
    "\n",
    "读取整个模型而不使用nn.DataParallel进行分布式训练设置\n",
    "\n",
    "这种情况可能不会报错，测试中发现程序会自动使用设备的前n个GPU进行训练（n是保存的模型使用的GPU个数）。此时如果指定的GPU个数少于n，则会报错。在这种情况下，只有保存模型时环境的device id和读取模型时环境的device id一致，程序才会按照预期在指定的GPU上进行分布式训练。\n",
    "\n",
    "相比之下，读取模型权重，之后再使用nn.DataParallel进行分布式训练设置则没有问题。因此多卡模式下建议使用权重的方式存储和读取模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'   #这里替换成希望使用的GPU编号\n",
    "\n",
    "model = models.resnet152(pretrained=True)\n",
    "model = nn.DataParallel(model).cuda()\n",
    "\n",
    "# 保存+读取模型权重，强烈建议！！\n",
    "torch.save(model.state_dict(), save_dir)\n",
    "loaded_model = models.resnet152()   #注意这里需要对模型结构有定义\n",
    "loaded_model.load_state_dict(torch.load(save_dir)))\n",
    "loaded_model = nn.DataParallel(loaded_model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果只有保存的整个模型，也可以采用提取权重的方式构建新的模型\n",
    "# 读取整个模型\n",
    "loaded_whole_model = torch.load(save_dir)\n",
    "loaded_model = models.resnet152()   #注意这里需要对模型结构有定义\n",
    "loaded_model.state_dict = loaded_whole_model.state_dict\n",
    "loaded_model = nn.DataParallel(loaded_model).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面所有对于loaded_model修改权重字典的形式都是通过赋值来实现的，在PyTorch中还可以通过\"load_state_dict\"函数来实现。因此在上面的所有示例中，我们使用了两种实现方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_state_dict(loaded_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 其他参数的保存和读取\n",
    "在深度学习项目里，有时候我们不仅仅需要保存模型的权重，还需要保存一些其他的参数，比如训练的epoch数、训练的loss，优化器的参数，动态调整学习策略的参数等等。这些参数可以通过字典的形式保存在一个文件里，然后在读取模型时一起读取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'args': args,\n",
    "    }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "epoch = checkpoint['epoch']\n",
    "args = checkpoint['args']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
