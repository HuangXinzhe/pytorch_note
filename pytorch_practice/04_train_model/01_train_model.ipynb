{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 层全连接网络，先不使用非线性激活函数，每层的权重初始化为服从 \n",
    " 的正态分布，输出数据使用随机初始化的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_tools\n",
    "def set_seed(seed=1):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from common_tools import set_seed\n",
    "\n",
    "set_seed(1)  # 设置随机种子\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_num, layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linears = nn.ModuleList(\n",
    "            [nn.Linear(neural_num, neural_num, bias=False) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "\n",
    "            # 在forward()函数中判断每一次前向传播的输出的标准差是否为 nan，如果是 nan 则停止前向传播。\n",
    "            print(\"layer:{}, std:{}\".format(i, x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break\n",
    "        return x\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            # 判断这一层是否为线性层，如果为线性层则初始化权值\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # nn.init.normal_(m.weight.data, std=np.sqrt(1/self.neural_num))    # normal: mean=0, std=1\n",
    "                # nn.init.normal_(m.weight.data)    # normal: mean=0, std=1\n",
    "\n",
    "                # a = np.sqrt(6 / (self.neural_num + self.neural_num))\n",
    "                # # 把 a 变换到 tanh，计算增益\n",
    "                # tanh_gain = nn.init.calculate_gain('tanh')\n",
    "                # a *= tanh_gain\n",
    "                #\n",
    "                # nn.init.uniform_(m.weight.data, -a, a)\n",
    "\n",
    "                # nn.init.xavier_uniform_(m.weight.data, gain=tanh_gain)\n",
    "\n",
    "                # nn.init.normal_(m.weight.data, std=np.sqrt(2 / self.neural_num))\n",
    "                nn.init.normal_(m.weight.data)    # normal: mean=0, std=1\n",
    "\n",
    "\n",
    "layer_nums = 100\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "\n",
    "inputs = torch.randn((batch_size, neural_nums))  # normal: mean=0, std=1\n",
    "\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 权值初始化\n",
    "常用初始化方法  \n",
    "\n",
    "PyTorch 中提供了 10 中初始化方法  \n",
    "- Xavier 均匀分布\n",
    "- Xavier 正态分布\n",
    "- Kaiming 均匀分布\n",
    "- Kaiming 正态分布\n",
    "- 均匀分布\n",
    "- 正态分布\n",
    "- 常数分布\n",
    "- 正交矩阵初始化\n",
    "- 单位矩阵初始化\n",
    "- 稀疏矩阵初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh_gain = nn.init.calculate_gain('tanh')\n",
    "nn.init.xavier_uniform_(m.weight.data, gain=tanh_gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
